Promptfoo Startup Performance Benchmark
=======================================

Running 5 iterations per command (after 1 warmup runs)

Benchmarking "--version"... avg: 601.02ms (min: 572.68ms, max: 647.21ms)
Benchmarking "--help"... avg: 590.79ms (min: 568.42ms, max: 614.31ms)
Benchmarking "eval --help"... avg: 1033.84ms (min: 670.59ms, max: 1707.42ms)
Benchmarking "init --help"... avg: 656.08ms (min: 606.99ms, max: 740.97ms)
Benchmarking "view --help"... avg: 735.97ms (min: 611.20ms, max: 1075.66ms)
Benchmarking "share --help"... avg: 739.13ms (min: 660.73ms, max: 847.57ms)
Benchmarking "cache --help"... avg: 837.93ms (min: 732.48ms, max: 1005.80ms)
Benchmarking "config --help"... avg: 962.91ms (min: 817.26ms, max: 1245.37ms)
Benchmarking "list --help"... avg: 847.74ms (min: 641.87ms, max: 1178.97ms)
Benchmarking "show --help"... avg: 898.80ms (min: 659.04ms, max: 1299.99ms)
Benchmarking "delete --help"... avg: 703.53ms (min: 672.58ms, max: 770.09ms)
Benchmarking "import --help"... avg: 746.95ms (min: 641.10ms, max: 994.32ms)
Benchmarking "export --help"... avg: 653.18ms (min: 627.49ms, max: 694.75ms)
Benchmarking "generate --help"... avg: 659.31ms (min: 610.17ms, max: 791.62ms)
Benchmarking "redteam --help"... avg: 616.18ms (min: 607.71ms, max: 626.42ms)
Benchmarking "eval (no args)"... avg: 895.96ms (min: 710.70ms, max: 1176.14ms)
Benchmarking "list evals"... avg: 1718.78ms (min: 1304.66ms, max: 2850.90ms)


Markdown Table for TODO.md:
| Command | Average (ms) | Min (ms) | Max (ms) |
|---------|-------------|----------|----------|
| --version | 601.02 | 572.68 | 647.21 |
| --help | 590.79 | 568.42 | 614.31 |
| eval --help | 1033.84 | 670.59 | 1707.42 |
| init --help | 656.08 | 606.99 | 740.97 |
| view --help | 735.97 | 611.20 | 1075.66 |
| share --help | 739.13 | 660.73 | 847.57 |
| cache --help | 837.93 | 732.48 | 1005.80 |
| config --help | 962.91 | 817.26 | 1245.37 |
| list --help | 847.74 | 641.87 | 1178.97 |
| show --help | 898.80 | 659.04 | 1299.99 |
| delete --help | 703.53 | 672.58 | 770.09 |
| import --help | 746.95 | 641.10 | 994.32 |
| export --help | 653.18 | 627.49 | 694.75 |
| generate --help | 659.31 | 610.17 | 791.62 |
| redteam --help | 616.18 | 607.71 | 626.42 |
| eval (no args) | 895.96 | 710.70 | 1176.14 |
| list evals | 1718.78 | 1304.66 | 2850.90 |

Raw results saved to: /Users/mdangelo/projects/promptfoo/scripts/benchmark-results-2025-07-12T17-02-30-672Z.json
