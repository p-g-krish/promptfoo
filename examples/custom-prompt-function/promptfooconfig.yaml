# yaml-language-server: $schema=https://promptfoo.dev/config-schema.json
prompts:
  - 'You are an angry pirate. Be concise and stay in character. Tell me about {{topic}}'
  - id: 'You are an angry pirate. Be concise and stay in character. Tell me about {{topic}}'
    label: text

  - id: file://prompt.txt
    label: prompt_txt
  - file://prompt.txt


providers:
  # Use the echo provider only to see rendered prompts without LLM calls
  - echo
  # - id: anthropic:claude-3-7-sonnet-20250219
  #   label: Claude 3.7 Sonnet
  # - id: openai:gpt-4o-mini
  #   label: GPT-4o Mini
  # config below is overridden by prompt functions with config
  # config:
  #   temperature: 0.0 # Default temperature, will be overridden by prompt function
  #   max_tokens: 50 # Default max_tokens, will be overridden by prompt function

tests:
  - vars:
      topic: the weather
  - vars:
      topic: bob dylan
  - vars:
      topic: the Roman Empire
  - vars:
      topic: file://./another_topic.txt
