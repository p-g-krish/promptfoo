Promptfoo Startup Performance Benchmark
=======================================

Running 5 iterations per command (after 1 warmup runs)

Benchmarking "--version"... avg: 469.65ms (min: 331.98ms, max: 862.37ms)
Benchmarking "--help"... avg: 431.20ms (min: 375.32ms, max: 575.06ms)
Benchmarking "eval --help"... avg: 369.26ms (min: 355.37ms, max: 376.18ms)
Benchmarking "init --help"... avg: 521.15ms (min: 357.68ms, max: 716.32ms)
Benchmarking "view --help"... avg: 365.95ms (min: 353.00ms, max: 394.52ms)
Benchmarking "share --help"... avg: 414.03ms (min: 360.22ms, max: 532.54ms)
Benchmarking "cache --help"... avg: 399.07ms (min: 335.67ms, max: 490.57ms)
Benchmarking "config --help"... avg: 427.54ms (min: 335.57ms, max: 500.32ms)
Benchmarking "list --help"... avg: 676.13ms (min: 430.80ms, max: 1344.06ms)
Benchmarking "show --help"... avg: 541.87ms (min: 417.11ms, max: 757.52ms)
Benchmarking "delete --help"... avg: 553.07ms (min: 426.15ms, max: 690.60ms)
Benchmarking "import --help"... avg: 487.72ms (min: 366.60ms, max: 799.32ms)
Benchmarking "export --help"... avg: 390.57ms (min: 340.76ms, max: 442.40ms)
Benchmarking "generate --help"... avg: 523.77ms (min: 377.50ms, max: 818.53ms)
Benchmarking "redteam --help"... avg: 584.60ms (min: 366.51ms, max: 1139.16ms)
Benchmarking "eval (no args)"... avg: 859.34ms (min: 653.47ms, max: 1184.32ms)
Benchmarking "list evals"... avg: 489.26ms (min: 414.72ms, max: 735.02ms)


Markdown Table for TODO.md:
| Command | Average (ms) | Min (ms) | Max (ms) |
|---------|-------------|----------|----------|
| --version | 469.65 | 331.98 | 862.37 |
| --help | 431.20 | 375.32 | 575.06 |
| eval --help | 369.26 | 355.37 | 376.18 |
| init --help | 521.15 | 357.68 | 716.32 |
| view --help | 365.95 | 353.00 | 394.52 |
| share --help | 414.03 | 360.22 | 532.54 |
| cache --help | 399.07 | 335.67 | 490.57 |
| config --help | 427.54 | 335.57 | 500.32 |
| list --help | 676.13 | 430.80 | 1344.06 |
| show --help | 541.87 | 417.11 | 757.52 |
| delete --help | 553.07 | 426.15 | 690.60 |
| import --help | 487.72 | 366.60 | 799.32 |
| export --help | 390.57 | 340.76 | 442.40 |
| generate --help | 523.77 | 377.50 | 818.53 |
| redteam --help | 584.60 | 366.51 | 1139.16 |
| eval (no args) | 859.34 | 653.47 | 1184.32 |
| list evals | 489.26 | 414.72 | 735.02 |

Raw results saved to: /Users/mdangelo/projects/promptfoo/scripts/benchmark-results-2025-07-12T17-08-38-410Z.json
